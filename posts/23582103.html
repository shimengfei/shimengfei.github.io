<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>提示工程 | 飞飞 ❤️ 晨晨</title><meta name="keywords" content="技术"><meta name="author" content="shimengfei"><meta name="copyright" content="shimengfei"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="概述提示工程（Prompt Engineering）是一门较新的学科，关注提示词开发和优化，帮助用户将大语言模型（Large Language Model, LLM）用于各场景和研究领域。 提示工程不仅仅是关于设计和研发提示词。它包含了与大语言模型交互和研发的各种技能和技术。提示工程在实现和大语言模型交互、对接，以及理解大语言模型能力方面都起着重要作用。用户可以通过提示工程来提高大语言模型的安全性">
<meta property="og:type" content="article">
<meta property="og:title" content="提示工程">
<meta property="og:url" content="https://www.smfcc.cn/posts/23582103.html">
<meta property="og:site_name" content="飞飞 ❤️ 晨晨">
<meta property="og:description" content="概述提示工程（Prompt Engineering）是一门较新的学科，关注提示词开发和优化，帮助用户将大语言模型（Large Language Model, LLM）用于各场景和研究领域。 提示工程不仅仅是关于设计和研发提示词。它包含了与大语言模型交互和研发的各种技能和技术。提示工程在实现和大语言模型交互、对接，以及理解大语言模型能力方面都起着重要作用。用户可以通过提示工程来提高大语言模型的安全性">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2025-06-26T14:49:10.000Z">
<meta property="article:modified_time" content="2025-06-26T16:03:41.142Z">
<meta property="article:author" content="shimengfei">
<meta property="article:tag" content="技术">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/shimengfei/cdn/img/(0).jpg.webp"><link rel="canonical" href="https://www.smfcc.cn/posts/23582103"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?ad46175d21aa084eb0fe1012619fd025";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: shimengfei","link":"链接: ","source":"来源: 飞飞 ❤️ 晨晨","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-06-27 00:03:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/img/(0).jpg.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">17</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 生活</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comments"></i><span> 说说</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">飞飞 ❤️ 晨晨</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 生活</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-comments"></i><span> 说说</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">提示工程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-26T14:49:10.000Z" title="发表于 2025-06-26 22:49:10">2025-06-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-26T16:03:41.142Z" title="更新于 2025-06-27 00:03:41">2025-06-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>35分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/23582103.html#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/posts/23582103.html" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>提示工程（Prompt Engineering）是一门较新的学科，关注提示词开发和优化，帮助用户将大语言模型（Large Language Model, LLM）用于各场景和研究领域。</p>
<p>提示工程不仅仅是关于设计和研发提示词。它包含了与大语言模型交互和研发的各种技能和技术。提示工程在实现和大语言模型交互、对接，以及理解大语言模型能力方面都起着重要作用。用户可以通过提示工程来提高大语言模型的安全性，也可以赋能大语言模型，比如借助专业领域知识和外部工具来增强大语言模型能力。</p>
<p>比如</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">写一篇产品经理的演讲稿，注意使用以下词汇: 赋能,抓手,中台,闭环,落地,漏斗,沉淀,给到,同步,对齐,对标,迭代,拉通,打通,升级,交付,聚焦,倒逼,复盘,梳理,方案,联动,透传,咬合,洞察,渗透,兜底,解耦,耦合,复用,拆解</span><br></pre></td></tr></table></figure>


<p>提示词可以包含以下任意要素：</p>
<p><strong>指令</strong>：想要模型执行的特定任务或指令。</p>
<p><strong>上下文</strong>：包含外部信息或额外的上下文信息，引导语言模型更好地响应。</p>
<p><strong>输入数据</strong>：用户输入的内容或问题。</p>
<p><strong>输出指示</strong>：指定输出的类型或格式。</p>
<h1 id="提示技术"><a href="#提示技术" class="headerlink" title="提示技术"></a>提示技术</h1><h2 id="零样本提示"><a href="#零样本提示" class="headerlink" title="零样本提示"></a>零样本提示</h2><p>如今，经过大量数据训练并调整指令的LLM能够执行零样本任务：</p>
<p><em>提示：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">将文本分类为中性、负面或正面。文本：我认为这次假期还可以。情感：</span><br></pre></td></tr></table></figure>

<p><em>输出：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">中性</span><br></pre></td></tr></table></figure>

<h2 id="少样本提示"><a href="#少样本提示" class="headerlink" title="少样本提示"></a>少样本提示</h2><p>虽然大型语言模型展示了惊人的零样本能力，但在使用零样本设置时，它们在更复杂的任务上仍然表现不佳。少样本提示可以作为一种技术，以启用上下文学习，我们在提示中提供演示以引导模型实现更好的性能。演示作为后续示例的条件，我们希望模型生成响应。</p>
<p><em>提示：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">“whatpu”是坦桑尼亚的一种小型毛茸茸的动物。一个使用whatpu这个词的句子的例子是：我们在非洲旅行时看到了这些非常可爱的whatpus。“farduddle”是指快速跳上跳下。一个使用farduddle这个词的句子的例子是：</span><br></pre></td></tr></table></figure>

<p><em>输出：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当我们赢得比赛时，我们都开始庆祝跳跃。</span><br></pre></td></tr></table></figure>

<h2 id="链式思考（CoT）提示"><a href="#链式思考（CoT）提示" class="headerlink" title="链式思考（CoT）提示"></a>链式思考（CoT）提示</h2><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/cot_demo.png" alt="COT"></p>
<p><em>提示：</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">这组数中的奇数加起来是偶数：<span class="number">4</span>、<span class="number">8</span>、<span class="number">9</span>、<span class="number">15</span>、<span class="number">12</span>、<span class="number">2</span>、<span class="number">1</span>。</span><br><span class="line">A：将所有奇数相加（<span class="number">9</span>、<span class="number">15</span>、<span class="number">1</span>）得到<span class="number">25</span>。答案为False。</span><br><span class="line"></span><br><span class="line">这组数中的奇数加起来是偶数：<span class="number">17</span>、<span class="number">10</span>、<span class="number">19</span>、<span class="number">4</span>、<span class="number">8</span>、<span class="number">12</span>、<span class="number">24</span>。</span><br><span class="line">A：将所有奇数相加（<span class="number">17</span>、<span class="number">19</span>）得到<span class="number">36</span>。答案为True。</span><br><span class="line"></span><br><span class="line">这组数中的奇数加起来是偶数：<span class="number">16</span>、<span class="number">11</span>、<span class="number">14</span>、<span class="number">4</span>、<span class="number">8</span>、<span class="number">13</span>、<span class="number">24</span>。</span><br><span class="line">A：将所有奇数相加（<span class="number">11</span>、<span class="number">13</span>）得到<span class="number">24</span>。答案为True。</span><br><span class="line"></span><br><span class="line">这组数中的奇数加起来是偶数：<span class="number">17</span>、<span class="number">9</span>、<span class="number">10</span>、<span class="number">12</span>、<span class="number">13</span>、<span class="number">4</span>、<span class="number">2</span>。</span><br><span class="line">A：将所有奇数相加（<span class="number">17</span>、<span class="number">9</span>、<span class="number">13</span>）得到<span class="number">39</span>。答案为False。</span><br><span class="line"></span><br><span class="line">这组数中的奇数加起来是偶数：<span class="number">15</span>、<span class="number">32</span>、<span class="number">5</span>、<span class="number">13</span>、<span class="number">82</span>、<span class="number">7</span>、<span class="number">1</span>。</span><br><span class="line">A：</span><br></pre></td></tr></table></figure>

<p><em>输出：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">将所有奇数相加（15、5、13、7、1）得到41。答案为False。</span><br></pre></td></tr></table></figure>

<h3 id="零样本-COT-提示"><a href="#零样本-COT-提示" class="headerlink" title="零样本 COT 提示"></a>零样本 COT 提示</h3><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/zero_cot_demo.png" alt="Zero-shot COT"></p>
<p><em>提示：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？</span><br></pre></td></tr></table></figure>

<p><em>输出：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">11个苹果</span><br></pre></td></tr></table></figure>

<p>答案是不正确的！</p>
<p><em>提示：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？让我们逐步思考。</span><br></pre></td></tr></table></figure>

<p><em>输出：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">首先，您从10个苹果开始。您给了邻居和修理工各2个苹果，所以您还剩下6个苹果。然后您买了5个苹果，所以现在您有11个苹果。最后，您吃了1个苹果，所以您还剩下10个苹果。</span><br></pre></td></tr></table></figure>

<h3 id="自动思维链（Auto-CoT）"><a href="#自动思维链（Auto-CoT）" class="headerlink" title="自动思维链（Auto-CoT）"></a>自动思维链（Auto-CoT）</h3><p>当使用思维链提示时，这个过程需要手工制作有效且多样化的例子。这种手动工作可能会导致次优解决方案。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.03493">Zhang et al. （2022）(opens in a new tab)</a> 提出了一种消除人工的方法，即利用 LLMs “让我们一步一步地思考” 提示来生成一个接一个的推理链。这种自动过程仍然可能在生成的链中出现错误。为了减轻错误的影响，演示的多样性很重要。这项工作提出了Auto-CoT，它对具有多样性的问题进行采样，并生成推理链来构建演示。</p>
<p>Auto-CoT 主要由两个阶段组成：</p>
<ul>
<li>阶段1：<strong>问题聚类</strong>：将给定问题划分为几个聚类</li>
<li>阶段2：<strong>演示抽样</strong>：从每组数组中选择一个具有代表性的问题，并使用带有简单启发式的 Zero-Shot-CoT 生成其推理链</li>
</ul>
<p>简单的启发式方法可以是问题的长度（例如，60 个 tokens）和理由的步骤数（例如，5 个推理步骤）。这鼓励模型使用简单而准确的演示。</p>
<p>该过程如下图所示：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/auto_cot_demo.png" alt="AUTOCOT"></p>
<h3 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/amazon-science/auto-cot">Auto-CoT</a></li>
</ul>
<h2 id="自我一致性"><a href="#自我一致性" class="headerlink" title="自我一致性"></a>自我一致性</h2><p>自我一致性旨在“替换链式思维提示中使用的天真贪婪解码方法”。其想法是通过少样本 CoT 采样多个不同的推理路径，并使用生成结果选择最一致的答案。这有助于提高 CoT 提示在涉及算术和常识推理的任务中的性能。</p>
<p><em>提示：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？</span><br></pre></td></tr></table></figure>

<p><em>输出：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">35</span><br></pre></td></tr></table></figure>

<p>输出是错误的！如何通过自我一致性来改进这个问题？</p>
<p><em>提示：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Q：林中有15棵树。林业工人今天将在林中种树。完成后，将有21棵树。林业工人今天种了多少棵树？</span><br><span class="line">A：我们从15棵树开始。后来我们有21棵树。差异必须是他们种树的数量。因此，他们必须种了21-15 &#x3D; 6棵树。答案是6。</span><br><span class="line">Q：停车场有3辆汽车，又来了2辆汽车，停车场有多少辆汽车？</span><br><span class="line">A：停车场已经有3辆汽车。又来了2辆。现在有3 + 2 &#x3D; 5辆汽车。答案是5。</span><br><span class="line">Q：Leah有32块巧克力，她的姐姐有42块。如果他们吃了35块，他们总共还剩多少块？</span><br><span class="line">A：Leah有32块巧克力，Leah的姐姐有42块。这意味着最初有32 + 42 &#x3D; 74块巧克力。已经吃了35块。因此，他们总共还剩74-35 &#x3D; 39块巧克力。答案是39。</span><br><span class="line">Q：Jason有20个棒棒糖。他给Denny一些棒棒糖。现在Jason只有12个棒棒糖。Jason给Denny多少棒棒糖？</span><br><span class="line">A：Jason有20个棒棒糖。因为他现在只有12个，所以他必须把剩下的给Denny。他给Denny的棒棒糖数量必须是20-12 &#x3D; 8个棒棒糖。答案是8。</span><br><span class="line">Q：Shawn有五个玩具。圣诞节，他从他的父母那里得到了两个玩具。他现在有多少个玩具？</span><br><span class="line">A：他有5个玩具。他从妈妈那里得到了2个，所以在那之后他有5 + 2 &#x3D; 7个玩具。然后他从爸爸那里得到了2个，所以总共他有7 + 2 &#x3D; 9个玩具。答案是9。</span><br><span class="line">Q：服务器房间里有9台计算机。从周一到周四，每天都会安装5台计算机。现在服务器房间里有多少台计算机？</span><br><span class="line">A：从周一到周四有4天。每天都添加了5台计算机。这意味着总共添加了4 * 5 &#x3D;20台计算机。一开始有9台计算机，所以现在有9 + 20 &#x3D; 29台计算机。答案是29。</span><br><span class="line">Q：Michael有58个高尔夫球。星期二，他丢失了23个高尔夫球。星期三，他又丢失了2个。星期三结束时他还剩多少个高尔夫球？</span><br><span class="line">A：Michael最初有58个球。星期二他丢失了23个，所以在那之后他有58-23 &#x3D; 35个球。星期三他又丢失了2个，所以现在他有35-2 &#x3D; 33个球。答案是33。</span><br><span class="line">Q：Olivia有23美元。她用每个3美元的价格买了五个百吉饼。她还剩多少钱？</span><br><span class="line">A：她用每个3美元的价格买了5个百吉饼。这意味着她花了15美元。她还剩8美元。</span><br><span class="line">Q：当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？</span><br><span class="line">A：</span><br></pre></td></tr></table></figure>

<p><em>输出 1：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当我6岁时，我的妹妹是我的一半年龄，也就是3岁。现在我70岁了，所以她是70-3 &#x3D; 67岁。答案是67。</span><br></pre></td></tr></table></figure>

<p><em>输出 2：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当叙述者6岁时，他的妹妹是他年龄的一半，也就是3岁。现在叙述者70岁了，他的妹妹应该是70-3 &#x3D; 67岁。答案是67。</span><br></pre></td></tr></table></figure>

<p><em>输出 3：</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当我6岁时，我的妹妹是我的一半年龄，也就是3岁。现在我70岁了，所以她是70&#x2F;2 &#x3D; 35岁。答案是35。</span><br></pre></td></tr></table></figure>

<h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/qwen3_result.png" alt="image-20250626232345557"></p>
<h2 id="生成知识提示"><a href="#生成知识提示" class="headerlink" title="生成知识提示"></a>生成知识提示</h2><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/%E7%94%9F%E6%88%90%E7%9F%A5%E8%AF%86%E6%8F%90%E7%A4%BA.png" alt="GENKNOW"></p>
<p>对应的<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.08387">论文链接</a></p>
<h2 id="链式提示"><a href="#链式提示" class="headerlink" title="链式提示"></a>链式提示</h2><p>为了提高大语言模型的性能使其更可靠，一个重要的提示工程技术是将任务分解为许多子任务。 确定子任务后，将子任务的提示词提供给语言模型，得到的结果作为新的提示词的一部分。 这就是所谓的链式提示（prompt chaining），一个任务被分解为多个子任务，根据子任务创建一系列提示操作。</p>
<p>链式提示可以完成很复杂的任务。LLM 可能无法仅用一个非常详细的提示完成这些任务。在链式提示中，提示链对生成的回应执行转换或其他处理，直到达到期望结果。</p>
<p>除了提高性能，链式提示还有助于提高 LLM 应用的透明度，增加控制性和可靠性。这意味着您可以更容易地定位模型中的问题，分析并改进需要提高的不同阶段的性能。</p>
<p>提示链可以用于不同的场景，这些场景可能涉及多个操作或转换。例如，LLM 的一个常见用途是根据大型文本文档回答问题。想要更好阅读大文本文档，可以设计两个不同的提示，第一个提示负责提取相关引文以回答问题，第二个提示则以引文和原始文档为输入来回答给定的问题。换句话说，可以创建两个不同的提示来执行根据文档回答问题的任务。</p>
<p>提示 1:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">你是一个很有帮助的助手。你的任务是根据文档回答问题。第一步是从文档中提取与问题相关的引文，由####分隔。请使用&lt;quotes&gt;&lt;&#x2F;quotes&gt;输出引文列表。如果没有找到相关引文，请回应“未找到相关引文！”。</span><br><span class="line"></span><br><span class="line">####</span><br><span class="line">&#123;&#123;文档&#125;&#125;</span><br><span class="line">####</span><br></pre></td></tr></table></figure>

<p>提示 1 的输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;quotes&gt;</span><br><span class="line">- Chain-of-thought (CoT) prompting[27]</span><br><span class="line">- Generated knowledge prompting[37]</span><br><span class="line">- Least-to-most prompting[38]</span><br><span class="line">- Self-consistency decoding[39]</span><br><span class="line">- Complexity-based prompting[41]</span><br><span class="line">- Self-refine[42]</span><br><span class="line">- Tree-of-thought prompting[43]</span><br><span class="line">- Maieutic prompting[45]</span><br><span class="line">- Directional-stimulus prompting[46]</span><br><span class="line">- Textual inversion and embeddings[59]</span><br><span class="line">- Using gradient descent to search for prompts[61][62][63][64]</span><br><span class="line">- Prompt injection[65][66][67]</span><br><span class="line">&lt;&#x2F;quotes&gt;</span><br></pre></td></tr></table></figure>

<p>在第一个提示中返回的引文现在可以用作下面第二个提示的输入。您可以对这些引文进行清理，比如移除引用标志。可以在提示链中新建另一个提示来移除或使用这些引用标志，但现在您可以忽略这些。然后，第二个提示接收由第一个提示提取的相关引文，并根据文档和这些提取的引文生成一个有帮助的回答。第二个提示可以是以下内容：</p>
<p>提示 2：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">根据从文档中提取的相关引文（由&lt;quotes&gt;&lt;&#x2F;quotes&gt;分隔）和原始文档（由####分隔），请构建对问题的回答。请确保答案准确、语气友好且有帮助。</span><br><span class="line"></span><br><span class="line">####</span><br><span class="line">&#123;&#123;文档&#125;&#125;</span><br><span class="line">####</span><br><span class="line"></span><br><span class="line">&lt;quotes&gt;</span><br><span class="line">- Chain-of-thought (CoT) prompting[27]</span><br><span class="line">- Generated knowledge prompting[37]</span><br><span class="line">- Least-to-most prompting[38]</span><br><span class="line">- Self-consistency decoding[39]</span><br><span class="line">- Complexity-based prompting[41]</span><br><span class="line">- Self-refine[42]</span><br><span class="line">- Tree-of-thought prompting[43]</span><br><span class="line">- Maieutic prompting[45]</span><br><span class="line">- Directional-stimulus prompting[46]</span><br><span class="line">- Textual inversion and embeddings[59]</span><br><span class="line">- Using gradient descent to search for prompts[61][62][63][64]</span><br><span class="line">- Prompt injection[65][66][67]</span><br><span class="line">&lt;&#x2F;quotes&gt;</span><br></pre></td></tr></table></figure>



<p>提示 2 的输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">文档中提到的提示技术包括：</span><br><span class="line"></span><br><span class="line">1. Chain-of-thought (CoT) prompting[27]</span><br><span class="line">2. Generated knowledge prompting[37]</span><br><span class="line">3. Least-to-most prompting[38]</span><br><span class="line">4. Self-consistency decoding[39]</span><br><span class="line">5. Complexity-based prompting[41]</span><br><span class="line">6. Self-refine[42]</span><br><span class="line">7. Tree-of-thought prompting[43]</span><br><span class="line">8. Maieutic prompting[45]</span><br><span class="line">9. Directional-stimulus prompting[46]</span><br><span class="line">10. Textual inversion and embeddings[59]</span><br><span class="line">11. Using gradient descent to search for prompts[61][62][63][64]</span><br><span class="line">12. Prompt injection[65][66][67]</span><br><span class="line"></span><br><span class="line">每一种技术都采用独特的策略来增强或明确与大型语言模型的互动，以产生期望的结果。</span><br></pre></td></tr></table></figure>



<h2 id="思维树-ToT"><a href="#思维树-ToT" class="headerlink" title="思维树 (ToT)"></a>思维树 (ToT)</h2><p>对于需要探索或预判战略的复杂任务来说，传统或简单的提示技巧是不够的。最近，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.10601">Yao et el. (2023)(opens in a new tab)</a> 提出了思维树（Tree of Thoughts，ToT）框架，该框架基于思维链提示进行了总结，引导语言模型探索把思维作为中间步骤来解决通用问题。</p>
<p>ToT 维护着一棵思维树，思维由连贯的语言序列表示，这个序列就是解决问题的中间步骤。使用这种方法，LM 能够自己对严谨推理过程的中间思维进行评估。LM 将生成及评估思维的能力与搜索算法（如广度优先搜索和深度优先搜索）相结合，在系统性探索思维的时候可以向前验证和回溯。</p>
<p>ToT 框架原理如下：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/tot_demo.png" alt="TOT"></p>
<p>ToT 需要针对不同的任务定义思维/步骤的数量以及每步的候选项数量。例如，论文中的“算 24 游戏”是一种数学推理任务，需要分成 3 个思维步骤，每一步都需要一个中间方程。而每个步骤保留最优的（best） 5 个候选项。</p>
<p>ToT 完成算 24 的游戏任务要执行广度优先搜索（BFS），每步思维的候选项都要求 LM 给出能否得到 24 的评估：“sure/maybe/impossible”（一定能/可能/不可能） 。作者讲到：“目的是得到经过少量向前尝试就可以验证正确（sure）的局部解，基于‘太大/太小’的常识消除那些不可能（impossible）的局部解，其余的局部解作为‘maybe’保留。”每步思维都要抽样得到 3 个评估结果。整个过程如下图所示：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/TOT%E8%BF%87%E7%A8%8B.png" alt="TOT2"></p>
<p>从下图中报告的结果来看，ToT 的表现大大超过了其他提示方法：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/TOT%E7%BB%93%E6%9E%9C.png" alt="TOT3"></p>
<p>从大方向上来看，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.10601">Yao et el. (2023)(opens in a new tab)</a> 和 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.08291">Long (2023)(opens in a new tab)</a> 的核心思路是类似的。两种方法都是以多轮对话搜索树的形式来增强 LLM 解决复杂问题的能力。主要区别在于 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.10601">Yao et el. (2023)(opens in a new tab)</a> 采用了深度优先（DFS）/广度优先（BFS）/集束（beam）搜索，而 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.08291">Long (2023)(opens in a new tab)</a> 则提出由强化学习（Reinforcement Learning）训练出的 “ToT 控制器”（ToT Controller）来驱动树的搜索策略(包括什么时候回退和搜索到哪一级回退等等)。深度优先/广度优先/集束搜索是通用搜索策略，并不针对具体问题。相比之下，由强化学习训练出的 ToT 控制器有可能从新的数据集学习，或是在自对弈（AlphaGo vs. 蛮力搜索）的过程中学习。因此，即使采用的是冻结的 LLM，基于强化学习构建的 ToT 系统仍然可以不断进化，学习新的知识。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/dave1010/tree-of-thought-prompting">Hulbert (2023)(opens in a new tab)</a> 提出了思维树（ToT）提示法，将 ToT 框架的主要概念概括成了一段简短的提示词，指导 LLM 在一次提示中对中间思维做出评估。ToT 提示词的例子如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">假设三位不同的专家来回答这个问题。</span><br><span class="line">所有专家都写下他们思考这个问题的第一个步骤，然后与大家分享。</span><br><span class="line">然后，所有专家都写下他们思考的下一个步骤并分享。</span><br><span class="line">以此类推，直到所有专家写完他们思考的所有步骤。</span><br><span class="line">只要大家发现有专家的步骤出错了，就让这位专家离开。</span><br><span class="line">请问...</span><br></pre></td></tr></table></figure>



<h2 id="检索增强生成-RAG"><a href="#检索增强生成-RAG" class="headerlink" title="检索增强生成 (RAG)"></a>检索增强生成 (RAG)</h2><p>通用语言模型通过微调就可以完成几类常见任务，比如分析情绪和识别命名实体。这些任务不需要额外的背景知识就可以完成。</p>
<p>要完成更复杂和知识密集型的任务，可以基于语言模型构建一个系统，访问外部知识源来做到。这样的实现与事实更加一性，生成的答案更可靠，还有助于缓解“幻觉”问题。</p>
<p>Meta AI 的研究人员引入了一种叫做<a target="_blank" rel="noopener" href="https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/">检索增强生成（Retrieval Augmented Generation，RAG）(opens in a new tab)</a>的方法来完成这类知识密集型的任务。RAG 把一个信息检索组件和文本生成模型结合在一起。RAG 可以微调，其内部知识的修改方式很高效，不需要对整个模型进行重新训练。</p>
<p>RAG 会接受输入并检索出一组相关/支撑的文档，并给出文档的来源（例如维基百科）。这些文档作为上下文和输入的原始提示词组合，送给文本生成器得到最终的输出。这样 RAG 更加适应事实会随时间变化的情况。这非常有用，因为 LLM 的参数化知识是静态的。RAG 让语言模型不用重新训练就能够获取最新的信息，基于检索生成产生可靠的输出。</p>
<p>Lewis 等人（2021）提出一个通用的 RAG 微调方法。这种方法使用预训练的 seq2seq 作为参数记忆，用维基百科的密集向量索引作为非参数记忆（使通过神经网络预训练的检索器访问）。这种方法工作原理概况如下：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/rag_demo.png" alt="RAG"></p>
<p>RAG 在 <a target="_blank" rel="noopener" href="https://ai.google.com/research/NaturalQuestions">Natural Questions(opens in a new tab)</a>、<a target="_blank" rel="noopener" href="https://paperswithcode.com/dataset/webquestions">WebQuestions(opens in a new tab)</a> 和 CuratedTrec 等基准测试中表现抢眼。用 MS-MARCO 和 Jeopardy 问题进行测试时，RAG 生成的答案更符合事实、更具体、更多样。FEVER 事实验证使用 RAG 后也得到了更好的结果。</p>
<p>这说明 RAG 是一种可行的方案，能在知识密集型任务中增强语言模型的输出。</p>
<p>LangChain 代码示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Literal</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> WebBaseLoader</span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> langchain_core.vectorstores <span class="keyword">import</span> InMemoryVectorStore</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> START, StateGraph</span><br><span class="line"><span class="keyword">from</span> typing_extensions <span class="keyword">import</span> Annotated, List, TypedDict</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load and chunk contents of the blog</span></span><br><span class="line">loader = WebBaseLoader(</span><br><span class="line">    web_paths=(<span class="string">&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;</span>,),</span><br><span class="line">    bs_kwargs=<span class="built_in">dict</span>(</span><br><span class="line">        parse_only=bs4.SoupStrainer(</span><br><span class="line">            class_=(<span class="string">&quot;post-content&quot;</span>, <span class="string">&quot;post-title&quot;</span>, <span class="string">&quot;post-header&quot;</span>)</span><br><span class="line">        )</span><br><span class="line">    ),</span><br><span class="line">)</span><br><span class="line">docs = loader.load()</span><br><span class="line"></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">1000</span>, chunk_overlap=<span class="number">200</span>)</span><br><span class="line">all_splits = text_splitter.split_documents(docs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Update metadata (illustration purposes)</span></span><br><span class="line">total_documents = <span class="built_in">len</span>(all_splits)</span><br><span class="line">third = total_documents // <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, document <span class="keyword">in</span> <span class="built_in">enumerate</span>(all_splits):</span><br><span class="line">    <span class="keyword">if</span> i &lt; third:</span><br><span class="line">        document.metadata[<span class="string">&quot;section&quot;</span>] = <span class="string">&quot;beginning&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> i &lt; <span class="number">2</span> * third:</span><br><span class="line">        document.metadata[<span class="string">&quot;section&quot;</span>] = <span class="string">&quot;middle&quot;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        document.metadata[<span class="string">&quot;section&quot;</span>] = <span class="string">&quot;end&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Index chunks</span></span><br><span class="line">vector_store = InMemoryVectorStore(embeddings)</span><br><span class="line">_ = vector_store.add_documents(all_splits)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define schema for search</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">TypedDict</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Search query.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    query: Annotated[<span class="built_in">str</span>, ..., <span class="string">&quot;Search query to run.&quot;</span>]</span><br><span class="line">    section: Annotated[</span><br><span class="line">        Literal[<span class="string">&quot;beginning&quot;</span>, <span class="string">&quot;middle&quot;</span>, <span class="string">&quot;end&quot;</span>],</span><br><span class="line">        ...,</span><br><span class="line">        <span class="string">&quot;Section to query.&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define prompt for question-answering</span></span><br><span class="line">prompt = hub.pull(<span class="string">&quot;rlm/rag-prompt&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define state for application</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">State</span>(<span class="params">TypedDict</span>):</span></span><br><span class="line">    question: <span class="built_in">str</span></span><br><span class="line">    query: Search</span><br><span class="line">    context: List[Document]</span><br><span class="line">    answer: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">analyze_query</span>(<span class="params">state: State</span>):</span></span><br><span class="line">    structured_llm = llm.with_structured_output(Search)</span><br><span class="line">    query = structured_llm.invoke(state[<span class="string">&quot;question&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;query&quot;</span>: query&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieve</span>(<span class="params">state: State</span>):</span></span><br><span class="line">    query = state[<span class="string">&quot;query&quot;</span>]</span><br><span class="line">    retrieved_docs = vector_store.similarity_search(</span><br><span class="line">        query[<span class="string">&quot;query&quot;</span>],</span><br><span class="line">        <span class="built_in">filter</span>=<span class="keyword">lambda</span> doc: doc.metadata.get(<span class="string">&quot;section&quot;</span>) == query[<span class="string">&quot;section&quot;</span>],</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;context&quot;</span>: retrieved_docs&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">state: State</span>):</span></span><br><span class="line">    docs_content = <span class="string">&quot;\n\n&quot;</span>.join(doc.page_content <span class="keyword">for</span> doc <span class="keyword">in</span> state[<span class="string">&quot;context&quot;</span>])</span><br><span class="line">    messages = prompt.invoke(&#123;<span class="string">&quot;question&quot;</span>: state[<span class="string">&quot;question&quot;</span>], <span class="string">&quot;context&quot;</span>: docs_content&#125;)</span><br><span class="line">    response = llm.invoke(messages)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;answer&quot;</span>: response.content&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])</span><br><span class="line">graph_builder.add_edge(START, <span class="string">&quot;analyze_query&quot;</span>)</span><br><span class="line">graph = graph_builder.<span class="built_in">compile</span>()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> graph.stream(</span><br><span class="line">    &#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;What does the end of the post say about Task Decomposition?&quot;</span>&#125;,</span><br><span class="line">    stream_mode=<span class="string">&quot;updates&quot;</span>,</span><br><span class="line">):</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;step&#125;</span>\n\n----------------\n&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;analyze_query&#x27;</span>: &#123;<span class="string">&#x27;query&#x27;</span>: &#123;<span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;Task Decomposition&#x27;</span>, <span class="string">&#x27;section&#x27;</span>: <span class="string">&#x27;end&#x27;</span>&#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">----------------</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;retrieve&#x27;</span>: &#123;<span class="string">&#x27;context&#x27;</span>: [Document(<span class="built_in">id</span>=<span class="string">&#x27;d6cef137-e1e8-4ddc-91dc-b62bd33c6020&#x27;</span>, metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;https://lilianweng.github.io/posts/2023-06-23-agent/&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: <span class="number">39221</span>, <span class="string">&#x27;section&#x27;</span>: <span class="string">&#x27;end&#x27;</span>&#125;, page_content=<span class="string">&#x27;Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n\n\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.&#x27;</span>), Document(<span class="built_in">id</span>=<span class="string">&#x27;d1834ae1-eb6a-43d7-a023-08dfa5028799&#x27;</span>, metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;https://lilianweng.github.io/posts/2023-06-23-agent/&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: <span class="number">39086</span>, <span class="string">&#x27;section&#x27;</span>: <span class="string">&#x27;end&#x27;</span>&#125;, page_content=<span class="string">&#x27;&#125;\n]\nChallenges#\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:&#x27;</span>), Document(<span class="built_in">id</span>=<span class="string">&#x27;ca7f06e4-2c2e-4788-9a81-2418d82213d9&#x27;</span>, metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;https://lilianweng.github.io/posts/2023-06-23-agent/&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: <span class="number">32942</span>, <span class="string">&#x27;section&#x27;</span>: <span class="string">&#x27;end&#x27;</span>&#125;, page_content=<span class="string">&#x27;&#125;\n]\nThen after these clarification, the agent moved into the code writing mode with a different system message.\nSystem message:&#x27;</span>), Document(<span class="built_in">id</span>=<span class="string">&#x27;1fcc2736-30f4-4ef6-90f2-c64af92118cb&#x27;</span>, metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;https://lilianweng.github.io/posts/2023-06-23-agent/&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: <span class="number">35127</span>, <span class="string">&#x27;section&#x27;</span>: <span class="string">&#x27;end&#x27;</span>&#125;, page_content=<span class="string">&#x27;&quot;content&quot;: &quot;You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code\&#x27;s language, and CODE is the code:\\n\\nFILENAME\\n\`\`\`LANG\\nCODE\\n\`\`\`\\n\\nYou will start with the \\&quot;entrypoint\\&quot; file, then go to the ones that are imported by that file, and so on.\\nPlease&#x27;</span>)]&#125;&#125;</span><br><span class="line"></span><br><span class="line">----------------</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;generate&#x27;</span>: &#123;<span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.&#x27;</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">----------------</span><br></pre></td></tr></table></figure>

<h2 id="自动推理并使用工具-ART"><a href="#自动推理并使用工具-ART" class="headerlink" title="自动推理并使用工具 (ART)"></a>自动推理并使用工具 (ART)</h2><p>使用 LLM 完成任务时，交替运用 CoT 提示和工具已经被证明是一种即强大又稳健的方法。这类方法通常需要针对特定任务手写示范，还需要精心编写交替使用生成模型和工具的脚本。<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.09014">Paranjape et al., (2023)(opens in a new tab)</a>提出了一个新框架，该框架使用冻结的 LLM 来自动生成包含中间推理步骤的程序。</p>
<p>ART（Automatic Reasoning and Tool-use）的工作原理如下：</p>
<ul>
<li>接到一个新任务的时候，从任务库中选择多步推理和使用工具的示范。</li>
<li>在测试中，调用外部工具时，先暂停生成，将工具输出整合后继续接着生成。</li>
</ul>
<p>ART 引导模型总结示范，将新任务进行拆分并在恰当的地方使用工具。ART 采用的是零样本形式。ART 还可以手动扩展，只要简单地更新任务和工具库就可以修正推理步骤中的错误或是添加新的工具。这个过程如下：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/art_demo.png" alt="ART"></p>
<h2 id="自动提示工程师（APE）"><a href="#自动提示工程师（APE）" class="headerlink" title="自动提示工程师（APE）"></a>自动提示工程师（APE）</h2><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/ape_demo_1.png" alt="APE"></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.01910">Zhou等人，（2022）(opens in a new tab)</a> 提出了自动提示工程师 （APE），这是一个用于自动指令生成和选择的框架。指令生成问题被构建为自然语言合成问题，使用 LLMs 作为黑盒优化问题的解决方案来生成和搜索候选解。</p>
<p>第一步涉及一个大型语言模型（作为推理模型），该模型接收输出演示以生成任务的指令候选项。这些候选解将指导搜索过程。使用目标模型执行指令，然后根据计算的评估分数选择最合适的指令。</p>
<p>APE 发现了一个比人工设计的“让我们一步一步地思考”提示更好的零样本 CoT 提示 （<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.11916">Kojima 等人，2022(opens in a new tab)</a>）。</p>
<p>提示“让我们一步一步地解决这个问题，以确保我们有正确的答案。”引发了思维链的推理，并提高了 MultiArith 和 GSM8K 基准测试的性能：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/ape_demo.png" alt="APECOT"></p>
<h2 id="Active-Prompt"><a href="#Active-Prompt" class="headerlink" title="Active-Prompt"></a>Active-Prompt</h2><p>思维链（CoT）方法依赖于一组固定的人工注释范例。问题在于，这些范例可能不是不同任务的最有效示例。为了解决这个问题，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.12246.pdf">Diao 等人（2023）(opens in a new tab)</a>最近提出了一种新的提示方法，称为 Active-Prompt，以适应 LLMs 到不同的任务特定示例提示（用人类设计的 CoT 推理进行注释）。</p>
<p>下面是该方法的说明。第一步是使用或不使用少量 CoT 示例查询 LLM。对一组训练问题生成 <em>k</em> 个可能的答案。基于 <em>k</em> 个答案计算不确定度度量（使用不一致性）。选择最不确定的问题由人类进行注释。然后使用新的注释范例来推断每个问题。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/Active-Prompt.png" alt="ACTIVE"></p>
<h2 id="方向性刺激提示"><a href="#方向性刺激提示" class="headerlink" title="方向性刺激提示"></a>方向性刺激提示</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.11520">Li 等人，（2023）(opens in a new tab)</a>提出了一种新的提示技术，以更好地指导 LLM 生成所需的摘要。</p>
<p>训练了一个可调节的策略 LM 来生成刺激/提示。越来越多地使用RL来优化 LLM。</p>
<p>下图显示了方向性刺激提示与标准提示的比较。策略 LM 可以很小，并且可以优化以生成指导黑盒冻结 LLM 的提示。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/%E6%96%B9%E5%90%91%E6%80%A7%E5%88%BA%E6%BF%80%E6%8F%90%E7%A4%BA.png" alt="DSP"></p>
<h2 id="PAL（程序辅助语言模型）"><a href="#PAL（程序辅助语言模型）" class="headerlink" title="PAL（程序辅助语言模型）"></a>PAL（程序辅助语言模型）</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.10435">Gao 等人（2022）(opens in a new tab)</a>提出了一种使用 LLMs 读取自然语言问题并生成程序作为中间推理步骤的方法。被称为程序辅助语言模型（PAL），它与思维链提示不同，因为它不是使用自由形式文本来获得解决方案，而是将解决步骤卸载到类似 Python 解释器的编程运行时中。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/pal.png" alt="PAL"></p>
<p>图片来源：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.10435">Gao 等人（2022）(opens in a new tab)</a></p>
<p>让我们以 LangChain 和 OpenAI GPT-3 为例。我们有兴趣开发一个简单的应用程序，它能够解释所提出的问题，并利用 Python 解释器提供答案。</p>
<p>具体来说，我们有兴趣创建一个功能，允许使用 LLM 回答需要日期理解的问题。我们将为 LLM 提供一个提示，其中包括一些示例，这些示例是从<a target="_blank" rel="noopener" href="https://github.com/reasoning-machines/pal/blob/main/pal/prompt/date_understanding_prompt.py">这里(opens in a new tab)</a>采用的。</p>
<p>这是我们需要导入的包：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> dateutil.relativedelta <span class="keyword">import</span> relativedelta</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br></pre></td></tr></table></figure>

<p>配置环境：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">load_dotenv()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># API configuration</span></span><br><span class="line">openai.api_key = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># for LangChain</span></span><br><span class="line">os.environ[<span class="string">&quot;OPENAI_API_KEY&quot;</span>] = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>设置模型实例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">llm = OpenAI(model_name=<span class="string">&#x27;text-davinci-003&#x27;</span>, temperature=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>设置提示+问题：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">question &#x3D; &quot;Today is 27 February 2023. I was born exactly 25 years ago. What is the date I was born in MM&#x2F;DD&#x2F;YYYY?&quot;</span><br><span class="line"> </span><br><span class="line">DATE_UNDERSTANDING_PROMPT &#x3D; &quot;&quot;&quot;</span><br><span class="line"># Q: 2015 is coming in 36 hours. What is the date one week from today in MM&#x2F;DD&#x2F;YYYY?</span><br><span class="line"># If 2015 is coming in 36 hours, then today is 36 hours before.</span><br><span class="line">today &#x3D; datetime(2015, 1, 1) - relativedelta(hours&#x3D;36)</span><br><span class="line"># One week from today,</span><br><span class="line">one_week_from_today &#x3D; today + relativedelta(weeks&#x3D;1)</span><br><span class="line"># The answer formatted with %m&#x2F;%d&#x2F;%Y is</span><br><span class="line">one_week_from_today.strftime(&#39;%m&#x2F;%d&#x2F;%Y&#39;)</span><br><span class="line"># Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM&#x2F;DD&#x2F;YYYY?</span><br><span class="line"># If the first day of 2019 is a Tuesday, and today is the first Monday of 2019, then today is 6 days later.</span><br><span class="line">today &#x3D; datetime(2019, 1, 1) + relativedelta(days&#x3D;6)</span><br><span class="line"># The answer formatted with %m&#x2F;%d&#x2F;%Y is</span><br><span class="line">today.strftime(&#39;%m&#x2F;%d&#x2F;%Y&#39;)</span><br><span class="line"># Q: The concert was scheduled to be on 06&#x2F;01&#x2F;1943, but was delayed by one day to today. What is the date 10 days ago in MM&#x2F;DD&#x2F;YYYY?</span><br><span class="line"># If the concert was scheduled to be on 06&#x2F;01&#x2F;1943, but was delayed by one day to today, then today is one day later.</span><br><span class="line">today &#x3D; datetime(1943, 6, 1) + relativedelta(days&#x3D;1)</span><br><span class="line"># 10 days ago,</span><br><span class="line">ten_days_ago &#x3D; today - relativedelta(days&#x3D;10)</span><br><span class="line"># The answer formatted with %m&#x2F;%d&#x2F;%Y is</span><br><span class="line">ten_days_ago.strftime(&#39;%m&#x2F;%d&#x2F;%Y&#39;)</span><br><span class="line"># Q: It is 4&#x2F;19&#x2F;1969 today. What is the date 24 hours later in MM&#x2F;DD&#x2F;YYYY?</span><br><span class="line"># It is 4&#x2F;19&#x2F;1969 today.</span><br><span class="line">today &#x3D; datetime(1969, 4, 19)</span><br><span class="line"># 24 hours later,</span><br><span class="line">later &#x3D; today + relativedelta(hours&#x3D;24)</span><br><span class="line"># The answer formatted with %m&#x2F;%d&#x2F;%Y is</span><br><span class="line">today.strftime(&#39;%m&#x2F;%d&#x2F;%Y&#39;)</span><br><span class="line"># Q: Jane thought today is 3&#x2F;11&#x2F;2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM&#x2F;DD&#x2F;YYYY?</span><br><span class="line"># If Jane thought today is 3&#x2F;11&#x2F;2002, but today is in fact Mar 12, then today is 3&#x2F;12&#x2F;2002.</span><br><span class="line">today &#x3D; datetime(2002, 3, 12)</span><br><span class="line"># 24 hours later,</span><br><span class="line">later &#x3D; today + relativedelta(hours&#x3D;24)</span><br><span class="line"># The answer formatted with %m&#x2F;%d&#x2F;%Y is</span><br><span class="line">later.strftime(&#39;%m&#x2F;%d&#x2F;%Y&#39;)</span><br><span class="line"># Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM&#x2F;DD&#x2F;YYYY?</span><br><span class="line"># If Jane was born on the last day of Feburary in 2001 and today is her 16-year-old birthday, then today is 16 years later.</span><br><span class="line">today &#x3D; datetime(2001, 2, 28) + relativedelta(years&#x3D;16)</span><br><span class="line"># Yesterday,</span><br><span class="line">yesterday &#x3D; today - relativedelta(days&#x3D;1)</span><br><span class="line"># The answer formatted with %m&#x2F;%d&#x2F;%Y is</span><br><span class="line">yesterday.strftime(&#39;%m&#x2F;%d&#x2F;%Y&#39;)</span><br><span class="line"># Q: &#123;question&#125;</span><br><span class="line">&quot;&quot;&quot;.strip() + &#39;\n&#39;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">llm_out = llm(DATE_UNDERSTANDING_PROMPT.<span class="built_in">format</span>(question=question))</span><br><span class="line">print(llm_out)</span><br></pre></td></tr></table></figure>

<p>这将输出以下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># If today is 27 February 2023 and I was born exactly 25 years ago, then I was born 25 years before.</span><br><span class="line">today &#x3D; datetime(2023, 2, 27)</span><br><span class="line"># I was born 25 years before,</span><br><span class="line">born &#x3D; today - relativedelta(years&#x3D;25)</span><br><span class="line"># The answer formatted with %m&#x2F;%d&#x2F;%Y is</span><br><span class="line">born.strftime(&#39;%m&#x2F;%d&#x2F;%Y&#39;)</span><br></pre></td></tr></table></figure>

<p><code>llm_out</code> 是一段 <code>python</code> 代码，我们可以使用 <code>exec</code> 执行它：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">exec(llm_out)</span><br><span class="line">print(born)</span><br></pre></td></tr></table></figure>

<p>这将输出以下内容：<code>02/27/1998</code></p>
<h2 id="ReAct-框架"><a href="#ReAct-框架" class="headerlink" title="ReAct 框架"></a>ReAct 框架</h2><p>在<a href="https://www.smfcc.cn/posts/3c395b1.html">AI基础概念</a>里面已经说过基本原理，这里仅举例</p>
<h3 id="长链-ReAct-的使用"><a href="#长链-ReAct-的使用" class="headerlink" title="长链 ReAct 的使用"></a>长链 ReAct 的使用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%capture</span><br><span class="line"><span class="comment"># 更新或安装必要的库</span></span><br><span class="line">!pip install --upgrade openai</span><br><span class="line">!pip install --upgrade langchain</span><br><span class="line">!pip install --upgrade python-dotenv</span><br><span class="line">!pip install google-search-results</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 引入库</span></span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> load_tools</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> initialize_agent</span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line">load_dotenv()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 载入 API keys; 如果没有，你需要先获取。 </span></span><br><span class="line">os.environ[<span class="string">&quot;OPENAI_API_KEY&quot;</span>] = os.getenv(<span class="string">&quot;OPENAI_API_KEY&quot;</span>)</span><br><span class="line">os.environ[<span class="string">&quot;SERPER_API_KEY&quot;</span>] = os.getenv(<span class="string">&quot;SERPER_API_KEY&quot;</span>)</span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<p>现在我们可以配置 LLM，我们要用到的工具，以及允许我们将 ReAct 框架与 LLM 和其他工具结合使用的代理。请注意，我们使用搜索 API 来搜索外部信息，并使用 LLM 作为数学工具。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">llm = OpenAI(model_name=<span class="string">&quot;text-davinci-003&quot;</span> ,temperature=<span class="number">0</span>)</span><br><span class="line">tools = load_tools([<span class="string">&quot;google-serper&quot;</span>, <span class="string">&quot;llm-math&quot;</span>], llm=llm)</span><br><span class="line">agent = initialize_agent(tools, llm, agent=<span class="string">&quot;zero-shot-react-description&quot;</span>, verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>配置好之后，我们就可以用所需的查询或提示运行代理了。请注意，在这里，我们不会像论文中阐释的那样提供少样本的示例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">agent.run(<span class="string">&quot;奥利维亚·王尔德的男朋友是谁?他现在的年龄的0.23次方是多少?&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>链执行如下所示:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; 正在输入新代理执行器链......</span><br><span class="line">  我得查出奥利维亚·王尔德的男友是谁然后计算出他的年龄的 0.23 次方。</span><br><span class="line">操作: 搜索</span><br><span class="line">操作输入: “奥利维亚·王尔德的男友”</span><br><span class="line">观察: 奥利维亚·王尔德与杰森·苏代基斯在多年前订婚，在他们分手后，她开始与哈里·斯泰尔斯约会 — 参照他们的关系时间线。</span><br><span class="line">思考: 我需要找出哈里·斯泰尔斯的年龄。</span><br><span class="line">操作: 搜索</span><br><span class="line">操作输入: “哈里·斯泰尔斯的年龄”</span><br><span class="line">观察: 29 岁</span><br><span class="line">思考: 我需要计算 29 的 0.23 次方。</span><br><span class="line">操作: 计算器</span><br><span class="line">操作输入: 29^0.23</span><br><span class="line">观察: 答案: 2.169459462491557</span><br><span class="line"> </span><br><span class="line">思考: 现在我知道最终答案了。</span><br><span class="line">最终答案: 哈里·斯泰尔斯, 奥利维亚·王尔德的男朋友, 29 岁。他年龄的 0.23 次方是 2.169459462491557。</span><br><span class="line"> </span><br><span class="line">&gt; 结束链。</span><br></pre></td></tr></table></figure>

<p>我们得到如下输出:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;哈里·斯泰尔斯, 奥利维亚·王尔德的男朋友, 29 岁。他年龄的 0.23 次方是 2.169459462491557。&quot;</span><br></pre></td></tr></table></figure>

<p>参考<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/how_to/#agents">LangChain文档</a></p>
<h2 id="自我反思（Reflexion）"><a href="#自我反思（Reflexion）" class="headerlink" title="自我反思（Reflexion）"></a>自我反思（Reflexion）</h2><p>自我反思是一个通过语言反馈来强化基于语言的智能体的框架。根据 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.11366.pdf">Shinn et al. (2023)(opens in a new tab)</a>，“自我反思是一种‘口头’强化的新范例，它将策略参数化为智能体的记忆编码与 LLM 的参数选择配对。”</p>
<p>在高层次上，自我反思将来自环境的反馈（自由形式的语言或者标量）转换为语言反馈，也被称作 <strong>self-reflection</strong>，为下一轮中 LLM 智能体提供上下文。这有助于智能体快速有效地从之前的错误中学习，进而提升许多高级任务的性能。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/Reflexion_1.png" alt="&quot;自我反思框架&quot;"></p>
<p>如上图所示，自我反思由三个不同的模型组成：</p>
<ul>
<li><strong>参与者（Actor）</strong>：根据状态观测量生成文本和动作。参与者在环境中采取行动并接受观察结果，从而形成轨迹。<a target="_blank" rel="noopener" href="https://www.promptingguide.ai/techniques/cot">链式思考（CoT）(opens in a new tab)</a> 和 <a target="_blank" rel="noopener" href="https://www.promptingguide.ai/techniques/react">ReAct(opens in a new tab)</a> 被用作参与者模型。此外，还添加了记忆组件为智能体提供额外的上下文信息。</li>
<li><strong>评估者（Evaluator）</strong>：对参与者的输出进行评价。具体来说，它将生成的轨迹（也被称作短期记忆）作为输入并输出奖励分数。根据人物的不同，使用不同的奖励函数（决策任务使用LLM和基于规则的启发式奖励）。</li>
<li><strong>自我反思（Self-Reflection）</strong>：生成语言强化线索来帮助参与者实现自我完善。这个角色由大语言模型承担，能够为未来的试验提供宝贵的反馈。自我反思模型利用奖励信号、当前轨迹和其持久记忆生成具体且相关的反馈，并存储在记忆组件中。智能体利用这些经验（存储在长期记忆中）来快速改进决策。</li>
</ul>
<p>总的来说，自我反思的关键步骤是a)定义任务，b)生成轨迹，c)评估，d)执行自我反思，e)生成下一条轨迹。下图展示了自我反思的智能体学习迭代优化其行为来解决决策、编程和推理等各种人物的例子。自我反思（Refelxion）通过引入自我评估、自我反思和记忆组件来拓展 ReAct 框架。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn@1.3.9/img/Reflexion_2.png" alt="&quot;Reflexion 示例&quot;"></p>
<h3 id="何时自我反思？"><a href="#何时自我反思？" class="headerlink" title="何时自我反思？"></a>何时自我反思？</h3><p>自我反思最适合以下情况：</p>
<ol>
<li><strong>智能体需要从尝试和错误中学习</strong>：自我反思旨在通过反思过去的错误并将这些知识纳入未来的决策来帮助智能体提高表现。这非常适合智能体需要通过反复试验来学习的任务，例如决策、推理和编程。</li>
<li><strong>传统的强化学习方法失效</strong>：传统的强化学习（RL）方法通常需要大量的训练数据和昂贵的模型微调。自我反思提供了一种轻量级替代方案，不需要微调底层语言模型，从而使其在数据和计算资源方面更加高效。</li>
<li><strong>需要细致入微的反馈</strong>：自我反思利用语言反馈，这比传统强化学习中使用的标量奖励更加细致和具体。这让智能体能够更好地了解自己的错误，并在后续的试验中做出更有针对性的改进。</li>
<li><strong>可解释性和直接记忆很重要</strong>：与传统的强化学习方法相比，自我反思提供了一种更可解释、更直接的情景记忆形式。智能体的自我反思存储在其记忆组件中，让分析和理解其学习过程变得更加简单。</li>
</ol>
<p>自我反思在以下任务中是有效的：</p>
<ul>
<li><strong>序列决策</strong>：自我反思提高了智能体在 AlfWorld 任务中的表现，涉及在各种环境中导航并完成多步目标。</li>
<li><strong>推理</strong>：自我反思提高了 HotPotQA 上智能体的性能，HotPotQA 是一个需要对多个文档进行推理的问答数据集。</li>
<li><strong>编程</strong>：自我反思的智能体在 HumanEval 和 MBPP 等基准测试上编写出了更好的代码，在某些情况下实现 SOTA 结果。</li>
</ul>
<p>以下是自我反思的一些限制：</p>
<ul>
<li><strong>依赖自我评估能力</strong>：反思依赖于智能体准确评估其表现并产生有用反思的能力。这可能是具有挑战性的，尤其是对于复杂的任务，但随着模型功能的不断改进，预计自我反思会随着时间的推移而变得更好。</li>
<li><strong>长期记忆限制</strong>：自我反思使用最大容量的滑动窗口，但对于更复杂的任务，使用向量嵌入或 SQL 数据库等高级结构可能会更有利。</li>
<li><strong>代码生成限制</strong>：测试驱动开发在指定准确的输入输出映射方面存在限制（例如，受硬件影响的非确定性生成器函数和函数输出）。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">shimengfei</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.smfcc.cn/posts/23582103.html">https://www.smfcc.cn/posts/23582103.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.smfcc.cn" target="_blank">飞飞 ❤️ 晨晨</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/3c395b1.html"><img class="next-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/guidao/20.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">AI 基础概念介绍</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/3c395b1.html" title="AI 基础概念介绍"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/guidao/20.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-23</div><div class="title">AI 基础概念介绍</div></div></a></div><div><a href="/posts/9458b181.html" title="CockroachDB分布式事务解析"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/guidao/20.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-28</div><div class="title">CockroachDB分布式事务解析</div></div></a></div><div><a href="/posts/e65f8f8e.html" title="Golang Mutex源码分析"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/guidao/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-29</div><div class="title">Golang Mutex源码分析</div></div></a></div><div><a href="/posts/8d0aa32e.html" title="Golang sync 包分析"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/guidao/2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-24</div><div class="title">Golang sync 包分析</div></div></a></div><div><a href="/posts/99a0d2d3.html" title="MySQL Online DDL 原理"><img class="cover" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/guidao/8.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-27</div><div class="title">MySQL Online DDL 原理</div></div></a></div><div><a href="/posts/80ca231b.html" title="追源索骥：透过源码看懂Flink核心框架的执行流程"><img class="cover" data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="title">追源索骥：透过源码看懂Flink核心框架的执行流程</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/img/(0).jpg.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">shimengfei</div><div class="author-info__description">飞飞 ❤️ 晨晨</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">17</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/shimengfei"><i class="fab fa-github"></i><span>Github 主页</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/shimengfei" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:13001306383@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">当下的奋进与堕落，皆是一种积淀，它们会默默为你铺路</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%E6%8A%80%E6%9C%AF"><span class="toc-number">2.</span> <span class="toc-text">提示技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6%E6%A0%B7%E6%9C%AC%E6%8F%90%E7%A4%BA"><span class="toc-number">2.1.</span> <span class="toc-text">零样本提示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%91%E6%A0%B7%E6%9C%AC%E6%8F%90%E7%A4%BA"><span class="toc-number">2.2.</span> <span class="toc-text">少样本提示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%80%9D%E8%80%83%EF%BC%88CoT%EF%BC%89%E6%8F%90%E7%A4%BA"><span class="toc-number">2.3.</span> <span class="toc-text">链式思考（CoT）提示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%B6%E6%A0%B7%E6%9C%AC-COT-%E6%8F%90%E7%A4%BA"><span class="toc-number">2.3.1.</span> <span class="toc-text">零样本 COT 提示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%80%9D%E7%BB%B4%E9%93%BE%EF%BC%88Auto-CoT%EF%BC%89"><span class="toc-number">2.3.2.</span> <span class="toc-text">自动思维链（Auto-CoT）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99"><span class="toc-number">2.3.3.</span> <span class="toc-text">相关资料</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%88%91%E4%B8%80%E8%87%B4%E6%80%A7"><span class="toc-number">2.4.</span> <span class="toc-text">自我一致性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number">2.4.1.</span> <span class="toc-text">附录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E7%9F%A5%E8%AF%86%E6%8F%90%E7%A4%BA"><span class="toc-number">2.5.</span> <span class="toc-text">生成知识提示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%8F%90%E7%A4%BA"><span class="toc-number">2.6.</span> <span class="toc-text">链式提示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E7%BB%B4%E6%A0%91-ToT"><span class="toc-number">2.7.</span> <span class="toc-text">思维树 (ToT)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90-RAG"><span class="toc-number">2.8.</span> <span class="toc-text">检索增强生成 (RAG)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%8E%A8%E7%90%86%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%B7%A5%E5%85%B7-ART"><span class="toc-number">2.9.</span> <span class="toc-text">自动推理并使用工具 (ART)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E5%B8%88%EF%BC%88APE%EF%BC%89"><span class="toc-number">2.10.</span> <span class="toc-text">自动提示工程师（APE）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Active-Prompt"><span class="toc-number">2.11.</span> <span class="toc-text">Active-Prompt</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E5%90%91%E6%80%A7%E5%88%BA%E6%BF%80%E6%8F%90%E7%A4%BA"><span class="toc-number">2.12.</span> <span class="toc-text">方向性刺激提示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PAL%EF%BC%88%E7%A8%8B%E5%BA%8F%E8%BE%85%E5%8A%A9%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%89"><span class="toc-number">2.13.</span> <span class="toc-text">PAL（程序辅助语言模型）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ReAct-%E6%A1%86%E6%9E%B6"><span class="toc-number">2.14.</span> <span class="toc-text">ReAct 框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E9%93%BE-ReAct-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">2.14.1.</span> <span class="toc-text">长链 ReAct 的使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%88%91%E5%8F%8D%E6%80%9D%EF%BC%88Reflexion%EF%BC%89"><span class="toc-number">2.15.</span> <span class="toc-text">自我反思（Reflexion）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%95%E6%97%B6%E8%87%AA%E6%88%91%E5%8F%8D%E6%80%9D%EF%BC%9F"><span class="toc-number">2.15.1.</span> <span class="toc-text">何时自我反思？</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/23582103.html" title="提示工程"><img data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="提示工程"/></a><div class="content"><a class="title" href="/posts/23582103.html" title="提示工程">提示工程</a><time datetime="2025-06-26T14:49:10.000Z" title="发表于 2025-06-26 22:49:10">2025-06-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/3c395b1.html" title="AI 基础概念介绍"><img data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/guidao/20.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI 基础概念介绍"/></a><div class="content"><a class="title" href="/posts/3c395b1.html" title="AI 基础概念介绍">AI 基础概念介绍</a><time datetime="2025-06-23T15:21:06.000Z" title="发表于 2025-06-23 23:21:06">2025-06-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/84c62ab4.html" title="精要主义读后感(一)"><img data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="精要主义读后感(一)"/></a><div class="content"><a class="title" href="/posts/84c62ab4.html" title="精要主义读后感(一)">精要主义读后感(一)</a><time datetime="2025-02-23T11:10:25.000Z" title="发表于 2025-02-23 19:10:25">2025-02-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/f48cea97.html" title="数据结构的存储方式"><img data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构的存储方式"/></a><div class="content"><a class="title" href="/posts/f48cea97.html" title="数据结构的存储方式">数据结构的存储方式</a><time datetime="2024-12-03T15:03:17.000Z" title="发表于 2024-12-03 23:03:17">2024-12-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/99a0d2d3.html" title="MySQL Online DDL 原理"><img data-lazy-src="https://cdn.jsdelivr.net/gh/shimengfei/cdn/guidao/8.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MySQL Online DDL 原理"/></a><div class="content"><a class="title" href="/posts/99a0d2d3.html" title="MySQL Online DDL 原理">MySQL Online DDL 原理</a><time datetime="2024-11-27T15:14:24.000Z" title="发表于 2024-11-27 23:14:24">2024-11-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By shimengfei</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a href="https://beian.miit.gov.cn" style="color:#f72b07" target="_blank"><img class="icp-icon" src="https://beian.mps.gov.cn/img/logo01.dd7ff50e.png"><span>皖ICP备2025080519号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    let initData = {
      el: '#vcomment',
      appId: '664TGLdx3PsteAFf3ovNvEqY-gzGzoHsz',
      appKey: 'U7a1xIuFlp4o9Nn2OkwnfYNp',
      placeholder: '大家文明留言哈~',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }
    
    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script>(function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/f46f7ebf.js","daovoice")
</script><script>var isChatBtn = false
daovoice('init', {
  app_id: 'f46f7ebf',},{
  launcher: { 
     disableLauncherIcon: isChatBtn // 悬浮 ICON 是否显示
  },
});
daovoice('update');

if (isChatBtn) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      daovoice('show')
    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      daovoice('update', {},{
        launcher: { 
        disableLauncherIcon: true // 悬浮 ICON 是否显示
        },
      });
    }
    function chatBtnShow () {
      daovoice('update', {},{
        launcher: { 
        disableLauncherIcon: false // 悬浮 ICON 是否显示
        },
      });
    }
  }
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  if (typeof gtag === 'function') {
    gtag('config', '', {'page_path': window.location.pathname});
  }

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})


document.addEventListener('pjax:send', function () {
  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"live2d-widget-model-shizuku"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>